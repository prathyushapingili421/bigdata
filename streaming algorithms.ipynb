{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaf75aa6-809f-46cf-b2c3-e825085eab37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postings: 1842544 rows\nindustries: 422 rows\nskills: 35 rows\nbenefits: 67943 rows\njob_industries: 164808 rows\njob_skills: 213768 rows\nsalaries: 40785 rows\ncompanies: 95251 rows\ncompany_industries: 24375 rows\ncompany_specialities: 169387 rows\nemployee_counts: 35787 rows\n"
     ]
    }
   ],
   "source": [
    "# First, disable Delta format check since you explicitly want to read CSV files\n",
    "spark.conf.set(\"spark.databricks.delta.formatCheck.enabled\", \"false\")\n",
    "\n",
    "# Define file paths\n",
    "file_paths = {\n",
    "    \"postings\": \"/FileStore/tables/postings.csv\",\n",
    "    \"industries\": \"/FileStore/tables/industries.csv\",\n",
    "    \"skills\": \"/FileStore/tables/skills.csv\",\n",
    "    \"benefits\": \"/FileStore/tables/benefits.csv\",\n",
    "    \"job_industries\": \"/FileStore/tables/job_industries.csv\",\n",
    "    \"job_skills\": \"/FileStore/tables/job_skills.csv\",\n",
    "    \"salaries\": \"/FileStore/tables/salaries.csv\",\n",
    "    \"companies\": \"/FileStore/tables/companies.csv\",\n",
    "    \"company_industries\": \"/FileStore/tables/company_industries.csv\",\n",
    "    \"company_specialities\": \"/FileStore/tables/company_specialities.csv\",\n",
    "    \"employee_counts\": \"/FileStore/tables/employee_counts.csv\"\n",
    "}\n",
    "\n",
    "# Load datasets into Spark DataFrames with additional CSV options for better reliability\n",
    "dataframes = {}\n",
    "for name, path in file_paths.items():\n",
    "    try:\n",
    "        dataframes[name] = spark.read \\\n",
    "            .option(\"header\", True) \\\n",
    "            .option(\"inferSchema\", True) \\\n",
    "            .option(\"mode\", \"PERMISSIVE\") \\\n",
    "            .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") \\\n",
    "            .csv(path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {name}: {str(e)}\")\n",
    "\n",
    "# Access each DataFrame as needed\n",
    "postings_df = dataframes.get(\"postings\")\n",
    "industries_df = dataframes.get(\"industries\")\n",
    "skills_df = dataframes.get(\"skills\")\n",
    "benefits_df = dataframes.get(\"benefits\")\n",
    "job_industries_df = dataframes.get(\"job_industries\")\n",
    "job_skills_df = dataframes.get(\"job_skills\")\n",
    "salaries_df = dataframes.get(\"salaries\")\n",
    "companies_df = dataframes.get(\"companies\")\n",
    "company_industries_df = dataframes.get(\"company_industries\")\n",
    "company_specialities_df = dataframes.get(\"company_specialities\")\n",
    "employee_counts_df = dataframes.get(\"employee_counts\")\n",
    "\n",
    "# Verify that DataFrames were loaded successfully\n",
    "for name, df in dataframes.items():\n",
    "    if df is not None:\n",
    "        print(f\"{name}: {df.count()} rows\")\n",
    "    else:\n",
    "        print(f\"{name}: Failed to load\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ccbdf54-2e1c-48ae-b436-af55bd577a88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: DataFrame[job_id: string, company_name: string, title: string, description: string, max_salary: double, pay_period: string, location: string, company_id: string, views: int, med_salary: double, min_salary: double, formatted_work_type: string, applies: int, original_listed_time: timestamp, remote_allowed: boolean, job_posting_url: string, application_url: string, application_type: string, expiry: string, closed_time: timestamp, formatted_experience_level: string, skills_desc: string, listed_time: timestamp, posting_domain: string, sponsored: string, work_type: string, currency: string, compensation_type: string, normalized_salary: string, zip_code: string, fips: string]"
     ]
    }
   ],
   "source": [
    "# 1. Data Cleaning\n",
    "# Handle salary data - convert string to numeric and standardize\n",
    "from pyspark.sql.functions import col, when, regexp_replace, lower, trim, to_timestamp, datediff\n",
    "\n",
    "# Clean postings dataframe\n",
    "cleaned_postings_df = postings_df \\\n",
    "    .withColumn(\"max_salary\", col(\"max_salary\").cast(\"double\")) \\\n",
    "    .withColumn(\"min_salary\", col(\"min_salary\").cast(\"double\")) \\\n",
    "    .withColumn(\"med_salary\", col(\"med_salary\").cast(\"double\")) \\\n",
    "    .withColumn(\"views\", col(\"views\").cast(\"integer\")) \\\n",
    "    .withColumn(\"applies\", col(\"applies\").cast(\"integer\")) \\\n",
    "    .withColumn(\"remote_allowed\", when(lower(col(\"remote_allowed\")) == \"true\", True).otherwise(False)) \\\n",
    "    .withColumn(\"listed_time\", to_timestamp(\"listed_time\")) \\\n",
    "    .withColumn(\"closed_time\", to_timestamp(\"closed_time\")) \\\n",
    "    .withColumn(\"original_listed_time\", to_timestamp(\"original_listed_time\"))\n",
    "\n",
    "# Clean up description text\n",
    "cleaned_postings_df = cleaned_postings_df \\\n",
    "    .withColumn(\"description\", regexp_replace(col(\"description\"), \"[^a-zA-Z0-9\\\\s]\", \" \")) \\\n",
    "    .withColumn(\"description\", trim(regexp_replace(col(\"description\"), \"\\\\s+\", \" \")))\n",
    "\n",
    "# Remove any duplicate job postings\n",
    "cleaned_postings_df = cleaned_postings_df.dropDuplicates([\"job_id\"])\n",
    "\n",
    "# Remove rows with null values in critical columns\n",
    "critical_columns = [\"job_id\", \"title\", \"company_id\", \"description\"]\n",
    "cleaned_postings_df = cleaned_postings_df.dropna(subset=critical_columns)\n",
    "\n",
    "# Cache the cleaned dataframe as we'll use it multiple times\n",
    "cleaned_postings_df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00de109f-837c-484f-a9b3-097f20d03830",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Reservoir Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de88ed6d-543c-4551-88d5-b62188a38b44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "import random\n",
    "\n",
    "def reservoir_sampling(df, sample_size=1000):\n",
    "    \"\"\"\n",
    "    Implement reservoir sampling on a DataFrame\n",
    "    \"\"\"\n",
    "    # Convert to RDD for easier sampling\n",
    "    rdd = df.rdd\n",
    "    \n",
    "    def reservoir_sample_map(iterator):\n",
    "        reservoir = []\n",
    "        for i, item in enumerate(iterator):\n",
    "            if i < sample_size:\n",
    "                reservoir.append(item)\n",
    "            else:\n",
    "                j = random.randrange(i + 1)\n",
    "                if j < sample_size:\n",
    "                    reservoir[j] = item\n",
    "        return iter(reservoir)\n",
    "    \n",
    "    sampled_rdd = rdd.mapPartitions(reservoir_sample_map)\n",
    "    return spark.createDataFrame(sampled_rdd, df.schema)\n",
    "\n",
    "# Get a representative sample of job postings\n",
    "sampled_postings = reservoir_sampling(cleaned_postings_df, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "272ec1e5-75e9-4446-96da-aac902701339",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Bloom Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8e455c9-da7d-4354-b3dd-99af7201cdf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nTotal number of skills loaded: 35\nSample skills: ['ART', 'DSGN', 'ADVR', 'PRDM', 'DIST']\n\nTest Results:\n+-----------------+--------------+\n|            skill|is_known_skill|\n+-----------------+--------------+\n|              ART|          true|\n|NONEXISTENT_SKILL|         false|\n|             null|         false|\n+-----------------+--------------+\n\n\nMost Common Skills in Job Postings:\n+---------+--------------------+-----+\n|skill_abr|          skill_name|count|\n+---------+--------------------+-----+\n|       IT|Information Techn...|26137|\n|     SALE|               Sales|22475|\n|     MGMT|          Management|20861|\n|     MNFC|       Manufacturing|18185|\n|     HCPR|Health Care Provider|17369|\n|       BD|Business Development|14290|\n|      ENG|         Engineering|13009|\n|     OTHR|               Other|12608|\n|      FIN|             Finance| 8540|\n|     MRKT|           Marketing| 5525|\n+---------+--------------------+-----+\nonly showing top 10 rows\n\n\nTop Skills by Job Title:\n+--------------------+--------------------+-----+\n|               title|          skill_name|count|\n+--------------------+--------------------+-----+\n|       Sales Manager|               Sales|  668|\n|       Sales Manager|Business Development|  651|\n|     Project Manager|  Project Management|  301|\n|Administrative As...|      Administrative|  244|\n|   Senior Accountant| Accounting/Auditing|  235|\n|Customer Service ...|               Other|  233|\n| Executive Assistant|      Administrative|  224|\n|     Project Manager|Information Techn...|  220|\n|         Salesperson|               Sales|  211|\n|    Registered Nurse|Health Care Provider|  205|\n+--------------------+--------------------+-----+\nonly showing top 10 rows\n\n\nSkills Data Summary:\nTotal unique skills: 35\nTotal job-skill associations: 213768\nAverage skills per job: 1.69\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BooleanType, ArrayType, StringType\n",
    "import hashlib  # Using hashlib instead of mmh3 for compatibility\n",
    "\n",
    "\n",
    "class BloomFilter:\n",
    "    def __init__(self, size, num_hash_functions):\n",
    "        self.size = size\n",
    "        self.num_hash_functions = num_hash_functions\n",
    "        self.bit_array = [0] * size\n",
    "    \n",
    "    def _get_hash_values(self, item):\n",
    "        hash_values = []\n",
    "        for i in range(self.num_hash_functions):\n",
    "            value = f\"{item}_{i}\"\n",
    "            hash_value = int(hashlib.md5(value.encode()).hexdigest(), 16)\n",
    "            hash_values.append(hash_value % self.size)\n",
    "        return hash_values\n",
    "    \n",
    "    def add(self, item):\n",
    "        for index in self._get_hash_values(item):\n",
    "            self.bit_array[index] = 1\n",
    "    \n",
    "    def check(self, item):\n",
    "        return all(self.bit_array[index] == 1 for index in self._get_hash_values(item))\n",
    "\n",
    "# Create Bloom Filter for skills\n",
    "bloom_size = 10000\n",
    "num_hash_functions = 5\n",
    "skills_bloom = BloomFilter(bloom_size, num_hash_functions)\n",
    "\n",
    "# Add all skills to Bloom Filter using skill_abr (since that's what we have in our schema)\n",
    "skills_list = [row.skill_abr for row in skills_df.select(\"skill_abr\").collect()]\n",
    "print(f\"\\nTotal number of skills loaded: {len(skills_list)}\")\n",
    "print(\"Sample skills:\", skills_list[:5])\n",
    "\n",
    "for skill in skills_list:\n",
    "    if skill is not None:  # Add null check\n",
    "        skills_bloom.add(skill)\n",
    "\n",
    "# Create UDF for skill checking\n",
    "def check_skill(skill):\n",
    "    if skill is None:\n",
    "        return False\n",
    "    return skills_bloom.check(skill)\n",
    "\n",
    "check_skill_udf = udf(check_skill, BooleanType())\n",
    "\n",
    "# Let's test the Bloom Filter with some sample data\n",
    "test_df = spark.createDataFrame([\n",
    "    (skills_list[0],),  # Known skill\n",
    "    (\"NONEXISTENT_SKILL\",),  # Unknown skill\n",
    "    (None,),  # Null value\n",
    "], [\"skill\"])\n",
    "\n",
    "test_results = test_df.withColumn(\"is_known_skill\", check_skill_udf(\"skill\"))\n",
    "print(\"\\nTest Results:\")\n",
    "test_results.show()\n",
    "\n",
    "# Now let's analyze skills in job postings\n",
    "from pyspark.sql.functions import explode, split, lower, col\n",
    "\n",
    "# Create a DataFrame with job skills\n",
    "job_skills_analysis = job_skills_df \\\n",
    "    .join(skills_df, \"skill_abr\") \\\n",
    "    .groupBy(\"skill_abr\", \"skill_name\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc())\n",
    "\n",
    "print(\"\\nMost Common Skills in Job Postings:\")\n",
    "job_skills_analysis.show(10)\n",
    "\n",
    "# Optional: Save results for later use\n",
    "job_skills_analysis.write.mode(\"overwrite\").csv(\"/tmp/job_skills_analysis\")\n",
    "\n",
    "# Additional Analysis: Skills by Job Title\n",
    "job_title_skills = job_skills_df \\\n",
    "    .join(postings_df, \"job_id\") \\\n",
    "    .join(skills_df, \"skill_abr\") \\\n",
    "    .groupBy(\"title\", \"skill_name\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc())\n",
    "\n",
    "print(\"\\nTop Skills by Job Title:\")\n",
    "job_title_skills.show(10)\n",
    "\n",
    "# Create a summary of skills data\n",
    "print(\"\\nSkills Data Summary:\")\n",
    "print(f\"Total unique skills: {skills_df.count()}\")\n",
    "print(f\"Total job-skill associations: {job_skills_df.count()}\")\n",
    "print(f\"Average skills per job: {job_skills_df.groupBy('job_id').count().agg({'count': 'avg'}).collect()[0][0]:.2f}\")\n",
    "\n",
    "# Optional: Export results for Tableau visualization\n",
    "job_title_skills.write.mode(\"overwrite\").csv(\"/tmp/job_title_skills_analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9ba4c18-e0d3-4819-9534-a37ca274040c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DGIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ccc09b8-d4c7-45fb-9657-deeb7e515150",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n|skill_abr|count|\n+---------+-----+\n+---------+-----+\n\n+-----+------+-----------+\n|count| skill|window_days|\n+-----+------+-----------+\n|    0|PYTHON|          7|\n|    0|PYTHON|         30|\n|    0|PYTHON|         90|\n+-----+------+-----------+\n\n+---------+------------+----------------+------------+\n|skill_abr|recent_count|historical_count|growth_ratio|\n+---------+------------+----------------+------------+\n+---------+------------+----------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, unix_timestamp\n",
    "from collections import deque\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# First, let's clean and prepare the timestamps\n",
    "cleaned_skills_over_time = job_skills_df.join(cleaned_postings_df, \"job_id\") \\\n",
    "    .select(\n",
    "        \"skill_abr\",\n",
    "        unix_timestamp(\"listed_time\").cast(\"long\").alias(\"timestamp\")\n",
    "    ) \\\n",
    "    .filter(col(\"timestamp\").isNotNull()) \\\n",
    "    .orderBy(\"timestamp\")\n",
    "\n",
    "class DGIM:\n",
    "    def __init__(self, window_size):\n",
    "        self.window_size = window_size\n",
    "        self.buckets = deque()\n",
    "    \n",
    "    def add_bit(self, bit, timestamp):\n",
    "        # Remove outdated buckets\n",
    "        current_time = timestamp\n",
    "        while self.buckets and (current_time - self.buckets[0][1]) >= self.window_size:\n",
    "            self.buckets.popleft()\n",
    "        \n",
    "        if bit == 1:\n",
    "            # Add new bucket\n",
    "            self.buckets.append((1, current_time))\n",
    "            \n",
    "            # Merge buckets of same size\n",
    "            i = len(self.buckets) - 1\n",
    "            while i > 0 and self.buckets[i][0] == self.buckets[i-1][0]:\n",
    "                size = self.buckets[i][0]\n",
    "                self.buckets.pop()\n",
    "                self.buckets.pop()\n",
    "                self.buckets.append((size * 2, current_time))\n",
    "                i -= 2\n",
    "    \n",
    "    def count_ones(self, timestamp):\n",
    "        total = 0\n",
    "        for size, ts in self.buckets:\n",
    "            if timestamp - ts < self.window_size:\n",
    "                total += size\n",
    "        return total\n",
    "\n",
    "# Initialize DGIM\n",
    "window_size = 30 * 24 * 60 * 60  # 30 days in seconds\n",
    "dgim_tracker = DGIM(window_size)\n",
    "\n",
    "# Process skills mentions over time\n",
    "# Collect the data to process row by row (note: for large datasets, you might want to batch this)\n",
    "skill_timestamps = cleaned_skills_over_time.collect()\n",
    "\n",
    "# Track skill occurrences using DGIM\n",
    "for row in skill_timestamps:\n",
    "    if row.timestamp is not None:\n",
    "        dgim_tracker.add_bit(1, row.timestamp)\n",
    "\n",
    "# Function to get trending skills in last N days\n",
    "def get_trending_skills(days=30):\n",
    "    window_size = days * 24 * 60 * 60  # Convert days to seconds\n",
    "    current_time = int(datetime.now().timestamp())\n",
    "    \n",
    "    trending_skills = cleaned_skills_over_time \\\n",
    "        .filter(col(\"timestamp\") >= current_time - window_size) \\\n",
    "        .groupBy(\"skill_abr\") \\\n",
    "        .count() \\\n",
    "        .orderBy(col(\"count\").desc())\n",
    "    \n",
    "    return trending_skills\n",
    "\n",
    "# Get trending skills\n",
    "trending_skills_df = get_trending_skills()\n",
    "trending_skills_df.show(10)\n",
    "\n",
    "# Let's also create a function to analyze skill trends over different time windows\n",
    "def analyze_skill_trends(skill_abr, windows=[7, 30, 90]):\n",
    "    \"\"\"\n",
    "    Analyze skill trends over different time windows\n",
    "    windows: list of days to analyze\n",
    "    \"\"\"\n",
    "    current_time = int(datetime.now().timestamp())\n",
    "    \n",
    "    trends = []\n",
    "    for days in windows:\n",
    "        window_size = days * 24 * 60 * 60\n",
    "        count = cleaned_skills_over_time \\\n",
    "            .filter((col(\"timestamp\") >= current_time - window_size) & \n",
    "                   (col(\"skill_abr\") == skill_abr)) \\\n",
    "            .count()\n",
    "        \n",
    "        trends.append({\n",
    "            'window_days': days,\n",
    "            'skill': skill_abr,\n",
    "            'count': count\n",
    "        })\n",
    "    \n",
    "    return spark.createDataFrame(trends)\n",
    "\n",
    "# Example: Analyze trends for a specific skill\n",
    "# Replace 'PYTHON' with any skill abbreviation from your dataset\n",
    "skill_trends = analyze_skill_trends('PYTHON')\n",
    "skill_trends.show()\n",
    "\n",
    "# Let's also create a function to find emerging skills\n",
    "def find_emerging_skills(short_window=7, long_window=90):\n",
    "    \"\"\"\n",
    "    Find skills that are growing in popularity\n",
    "    by comparing recent activity to historical activity\n",
    "    \"\"\"\n",
    "    current_time = int(datetime.now().timestamp())\n",
    "    short_window_seconds = short_window * 24 * 60 * 60\n",
    "    long_window_seconds = long_window * 24 * 60 * 60\n",
    "    \n",
    "    # Recent activity\n",
    "    recent = cleaned_skills_over_time \\\n",
    "        .filter(col(\"timestamp\") >= current_time - short_window_seconds) \\\n",
    "        .groupBy(\"skill_abr\") \\\n",
    "        .count() \\\n",
    "        .withColumnRenamed(\"count\", \"recent_count\")\n",
    "    \n",
    "    # Historical activity\n",
    "    historical = cleaned_skills_over_time \\\n",
    "        .filter((col(\"timestamp\") >= current_time - long_window_seconds) &\n",
    "                (col(\"timestamp\") < current_time - short_window_seconds)) \\\n",
    "        .groupBy(\"skill_abr\") \\\n",
    "        .count() \\\n",
    "        .withColumnRenamed(\"count\", \"historical_count\")\n",
    "    \n",
    "    # Compare and find emerging skills\n",
    "    emerging_skills = recent.join(historical, \"skill_abr\") \\\n",
    "        .withColumn(\"growth_ratio\", \n",
    "                   (col(\"recent_count\") * (long_window/short_window)) / col(\"historical_count\")) \\\n",
    "        .orderBy(col(\"growth_ratio\").desc())\n",
    "    \n",
    "    return emerging_skills\n",
    "\n",
    "# Find emerging skills\n",
    "emerging_skills_df = find_emerging_skills()\n",
    "emerging_skills_df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7286d224-bd66-45fc-808d-8294047550ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting mmh3\n  Downloading mmh3-5.0.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\nInstalling collected packages: mmh3\nSuccessfully installed mmh3-5.0.1\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install mmh3"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Algorithms",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
